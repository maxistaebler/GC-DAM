{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from gliner import GLiNER\n",
    "import ollama\n",
    "from node2vec import Node2Vec\n",
    "import networkx as nx\n",
    "import plotly\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import ast\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GLiNER with the base model\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_mediumv2.1\")\n",
    "def create_splitter():\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=150,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    return splitter\n",
    "\n",
    "labels = [\n",
    "    \"person\", \"award\", \"date\", \"competitions\", \"teams\", \"organization\", \"location\", \"event\", \"product\", \n",
    "    \"quantity\", \"money\", \"percent\", \"time\", \"gpe\", \"facility\", \"language\", \"work_of_art\", \"law\", \"nationality\", \n",
    "    \"title\", \"field_of_study\", \"measurement\", \"technology\"\n",
    "]\n",
    "\n",
    "splitter = create_splitter()\n",
    "\n",
    "BUY_EMBEDDING_DF = \"../assets/buy_embeddings_stella_1.5b.csv\"\n",
    "ABT_EMBEDDING_DF = \"../assets/abt_embeddings_stella_1.5b.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download pre-trained embeddings\n",
    "\n",
    "[buy](https://drive.google.com/file/d/1kgsQWlAom-7iWgqi7gf6_uO6V-F33R0H/view?usp=sharing)\n",
    "\n",
    "[abt](https://drive.google.com/file/d/1gvBrm7quph1AIIC9t48QqQ0Wl0jGPJTm/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Entferne alle Satzzeichen (Punctuation)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Wandle den gesamten Text in Kleinbuchstaben um\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "abt_df = pd.read_csv('../datasets/abt_buy/Abt.csv', encoding='unicode_escape')\n",
    "abt_df.fillna('', inplace=True)\n",
    "abt_df['name'] = abt_df['name'].apply(lambda x: clean_text(x))\n",
    "abt_df['description'] = abt_df['description'].apply(lambda x: clean_text(x))\n",
    "\n",
    "buy_df = pd.read_csv('../datasets/abt_buy/Buy.csv', encoding='unicode_escape')\n",
    "buy_df.fillna('', inplace=True)\n",
    "buy_df['name'] = buy_df['name'].apply(lambda x: clean_text(x))\n",
    "buy_df['description'] = buy_df['description'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(abt_df))\n",
    "print(len(buy_df))\n",
    "abt_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buy_embeddings = []\n",
    "# for idx, row in tqdm(buy_df.iterrows()):\n",
    "#     t = f\"The Title is: {row['name']}. The price is: {row['price']}. And now the description: {row['description']}\"\n",
    "#     response = ollama.embeddings(model=\"Losspost/stella_en_1.5b_v5\", prompt=t)\n",
    "#     buy_embeddings.append(response['embedding'])\n",
    "# abt_embeddings = []\n",
    "# for idx, row in tqdm(abt_df.iterrows()):\n",
    "#     t = f\"The Title is: {row['name']}. The price is: {row['price']}. And now the description: {row['description']}\"\n",
    "#     response = ollama.embeddings(model=\"Losspost/stella_en_1.5b_v5\", prompt=t)\n",
    "#     abt_embeddings.append(response['embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_embd = pd.read_csv(BUY_EMBEDDING_DF)\n",
    "abt_embd = pd.read_csv(ABT_EMBEDDING_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ER Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_nodes = []\n",
    "\n",
    "for idx, row in tqdm(buy_df.iterrows()):\n",
    "    text = f\"The Title is: {row['name']}. The price is: {row['price']}. And now the description: {row['description']}\"\n",
    "    # Load Text\n",
    "    texts = splitter.create_documents([text])\n",
    "    # Split Text\n",
    "    pages = splitter.split_documents(texts)\n",
    "    for p in pages:\n",
    "        entities = model.predict_entities(p.page_content, labels, threshold=0.5)\n",
    "        for entity in entities:\n",
    "            buy_nodes.append((entity[\"text\"].lower(), entity[\"label\"].lower(), row['id'], 'google'))\n",
    "            # print(entity[\"text\"].lower(), \"=>\", entity[\"label\"].lower())\n",
    "\n",
    "print(len(list(set(buy_nodes))))\n",
    "\n",
    "#######\n",
    "\n",
    "abt_nodes = []\n",
    "\n",
    "for idx, row in tqdm(abt_df.iterrows()):\n",
    "    text = f\"The Title is: {row['name']}. The price is: {row['price']}. And now the description: {row['description']}\"\n",
    "    # Load Text\n",
    "    texts = splitter.create_documents([text])\n",
    "    # Split Text\n",
    "    pages = splitter.split_documents(texts)\n",
    "    for p in pages:\n",
    "        entities = model.predict_entities(p.page_content, labels, threshold=0.5)\n",
    "        for entity in entities:\n",
    "            abt_nodes.append((entity[\"text\"].lower(), entity[\"label\"].lower(), row['id'], 'amazon'))\n",
    "            # print(entity[\"text\"].lower(), \"=>\", entity[\"label\"].lower())\n",
    "\n",
    "print(len(list(set(abt_nodes))))\n",
    "\n",
    "nodes = buy_nodes + abt_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KG Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_edge_weight(G, u, v):\n",
    "    if G.has_edge(u, v):\n",
    "        G[u][v]['weight'] += 1\n",
    "    else:\n",
    "        # FÃ¼ge die Kante hinzu, falls sie noch nicht existiert, mit Gewicht 1\n",
    "        G.add_edge(u, v, weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Graph\n",
    "G = nx.Graph()\n",
    "## Add Nodes\n",
    "for node in nodes:\n",
    "    if not G.has_node(node[0]):\n",
    "        G.add_node(\n",
    "            node[0].lower(),\n",
    "            type=\"entity\"\n",
    "        )\n",
    "    if not G.has_node(node[1]):\n",
    "        G.add_node(\n",
    "            node[1].lower(),\n",
    "            type=\"label\"\n",
    "        )\n",
    "    if not G.has_node(node[2]):\n",
    "        G.add_node(\n",
    "            node[2],\n",
    "            type=\"id\"\n",
    "        )\n",
    "\n",
    "    # Add tuple edges\n",
    "    increment_edge_weight(G, node[0].lower(), node[1].lower())\n",
    "    increment_edge_weight(G, node[1].lower(), node[2])\n",
    "    increment_edge_weight(G, node[0].lower(), node[2])\n",
    "\n",
    "# Beispiel: Anzahl der Knoten und Kanten anzeigen\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.write_gexf(G, \"./assets/n2v_abt_buy_graph_full.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec = Node2Vec(G, dimensions=128, walk_length=40, num_walks=100, workers=2)\n",
    "model = node2vec.fit(window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('./assets/KG_abt_buy_n2v_embedding.csv')\n",
    "model.save('./assets/KG_abt_buy_n2v_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve node embeddings\n",
    "node_ids = model.wv.index_to_key  # list of node IDs\n",
    "node_labels = model.wv.key_to_index\n",
    "node_embeddings = model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2v_embd = pd.DataFrame(node_embeddings, index=node_labels).reset_index()\n",
    "n2v_embd.rename(columns = {'index':'product'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_idxs = [str(x) for x in buy_df['id']]\n",
    "abt_idxs = [str(x) for x in abt_df['id']]\n",
    "\n",
    "n2v_embd_buy= n2v_embd[n2v_embd['product'].isin(buy_idxs)]\n",
    "n2v_embd_abt = n2v_embd[n2v_embd['product'].isin(abt_idxs)]\n",
    "print(len(n2v_embd_buy))\n",
    "print(len(n2v_embd_abt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=list(n2v_embd_buy['product'].values) + list(n2v_embd_abt['product'].values)\n",
    "id_dataset = {('google' if str(x).split(':')[0] == 'http' else 'amazon'): k for k, x in enumerate(l)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2v_embd_buy['type'] = 'google'\n",
    "n2v_embd_abt['type'] = 'amazon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2v_combined = pd.concat([n2v_embd_abt, n2v_embd_buy])\n",
    "n2v_combined.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2v_vectors = np.array(n2v_combined.drop(['product', 'type'], axis=1).to_numpy())\n",
    "cosine_n2v = cosine_similarity(n2v_vectors)\n",
    "cosine_n2v_df = pd.DataFrame(cosine_n2v, index=n2v_combined['product'], columns=n2v_combined['product'])\n",
    "cosine_n2v_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google is True -->Y Invertieren fÃ¼r Amazon\n",
    "cosine_dataset_id = [True if x in buy_idxs else False for x in cosine_n2v_df.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_matching = pd.read_csv('../datasets/abt_buy/abt_buy_perfectMapping.csv')\n",
    "matching_scores = []\n",
    "\n",
    "for i, (idx, row) in enumerate(cosine_n2v_df.iterrows()):\n",
    "    pm_abt = perfect_matching[perfect_matching['idAbt']==int(idx)]['idBuy'].values\n",
    "    pm_abt = [str(x) for x in pm_abt]\n",
    "    pm_buy = perfect_matching[perfect_matching['idBuy']==int(idx)]['idAbt'].values\n",
    "    pm_buy = [str(x) for x in pm_buy]\n",
    "    if idx in buy_idxs:\n",
    "        f = pd.Series(cosine_dataset_id).values\n",
    "        d = cosine_n2v_df[f]\n",
    "        dd = d[d.index == idx].T\n",
    "        dd_f = dd[dd.index.isin(abt_idxs)].sort_values(idx, ascending=False)\n",
    "        indices = np.where(np.isin(dd_f.index.values, pm_buy))[0]\n",
    "        if len(indices) > 0:\n",
    "            tuple_out = [(x, dd_f.iloc[x][idx], 'buy', idx, pm_buy) for x in indices][0]\n",
    "            matching_scores.append(tuple_out)\n",
    "    elif idx in abt_idxs:\n",
    "        f = ~pd.Series(cosine_dataset_id).values\n",
    "        d = cosine_n2v_df[f]\n",
    "        dd = d[d.index == idx].T\n",
    "        dd_f = dd[dd.index.isin(buy_idxs)].sort_values(idx, ascending=False)\n",
    "        indices = np.where(np.isin(dd_f.index.values, pm_abt))[0]\n",
    "        if len(indices) > 0:\n",
    "            tuple_out = [(x, dd_f.iloc[x][idx], 'abt', idx, pm_abt) for x in indices][0]\n",
    "            matching_scores.append(tuple_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(matching_scores, columns=['rank', 'score', 'dataset', 'index', 'perfect_match'])\n",
    "print(f\"Ideal Matching (%): {round(result_df['rank'].value_counts()[0] / len(result_df), 3) * 100}\")\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_palette = sns.color_palette(['#FF33CC', '#39FF14', '#00FFFF', '#998650', '#E0BE36'])\n",
    "sns.set_palette(custom_palette)\n",
    "\n",
    "sns.set_style('ticks')\n",
    "sns.set_context('talk')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 7)\n",
    "\n",
    "sns.scatterplot(data=result_df, x=\"rank\", y=\"score\", hue='dataset')\n",
    "sns.rugplot(data=result_df, x=\"rank\", y=\"score\", color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_embd['type'] = 'google'\n",
    "abt_embd['type'] = 'amazon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_idxs = buy_embd['id'].values\n",
    "abt_idxs = abt_embd['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_embd_combined = pd.concat([buy_embd, abt_embd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_vectors = np.array(description_embd_combined.drop(['id', 'type'], axis=1).to_numpy())\n",
    "cosine_descriptions = cosine_similarity(description_vectors)\n",
    "cosine_descriptions_df = pd.DataFrame(cosine_descriptions, index=description_embd_combined['id'], columns=description_embd_combined['id'])\n",
    "cosine_descriptions_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_matching = pd.read_csv('../datasets/abt_buy/abt_buy_perfectMapping.csv')\n",
    "matching_scores = []\n",
    "\n",
    "cosine_dataset_id = [True if x in buy_idxs else False for x in cosine_descriptions_df.index.values]\n",
    "\n",
    "for i, (idx, row) in enumerate(cosine_descriptions_df.iterrows()):\n",
    "    pm_abt = perfect_matching[perfect_matching['idAbt']==idx]['idBuy'].values\n",
    "    pm_buy = perfect_matching[perfect_matching['idBuy']==idx]['idAbt'].values\n",
    "    if idx in buy_idxs:\n",
    "        f = pd.Series(cosine_dataset_id).values\n",
    "        d = cosine_descriptions_df[f]\n",
    "        dd = d[d.index == idx].T\n",
    "        dd_f = dd[dd.index.isin(abt_idxs)].sort_values(idx, ascending=False)\n",
    "        indices = np.where(np.isin(dd_f.index.values, pm_buy))[0]\n",
    "        if len(indices) > 0:\n",
    "            tuple_out = [(x, dd_f.iloc[x][idx], 'buy', idx, pm_buy) for x in indices][0]\n",
    "            matching_scores.append(tuple_out)\n",
    "    elif idx in abt_idxs:\n",
    "        f = ~pd.Series(cosine_dataset_id).values\n",
    "        d = cosine_descriptions_df[f]\n",
    "        dd = d[d.index == idx].T\n",
    "        dd_f = dd[dd.index.isin(buy_idxs)].sort_values(idx, ascending=False)\n",
    "        indices = np.where(np.isin(dd_f.index.values, pm_abt))[0]\n",
    "        if len(indices) > 0:\n",
    "            tuple_out = [(x, dd_f.iloc[x][idx], 'abt', idx, pm_abt) for x in indices][0]\n",
    "            matching_scores.append(tuple_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_description = pd.DataFrame(matching_scores, columns=['rank', 'score', 'dataset', 'index', 'perfect_match'])\n",
    "print(f\"Ideal Matching (%): {round(result_df_description['rank'].value_counts()[0] / len(result_df_description), 3) * 100}\")\n",
    "result_df_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_palette = sns.color_palette(['#FF33CC', '#39FF14', '#00FFFF', '#998650', '#E0BE36'])\n",
    "sns.set_palette(custom_palette)\n",
    "\n",
    "sns.set_style('ticks')\n",
    "sns.set_context('talk')\n",
    "fig, ax = plt.subplots()\n",
    "# sns.set_context(\"notebook\", rc={\"lines.linewidth\": 3})\n",
    "fig.set_size_inches(10, 7)\n",
    "\n",
    "sns.scatterplot(data=result_df_description, x=\"rank\", y=\"score\", hue='dataset')\n",
    "sns.rugplot(data=result_df_description, x=\"rank\", y=\"score\", color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kombination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = result_df.copy()\n",
    "final_result['rank'] = result_df['rank'].combine(result_df_description['rank'], min)\n",
    "final_result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ideal Matching (%): {round(final_result['rank'].value_counts()[0] / len(final_result), 3) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_palette = sns.color_palette(['#FF33CC', '#39FF14', '#00FFFF', '#998650', '#E0BE36'])\n",
    "sns.set_palette(custom_palette)\n",
    "\n",
    "sns.set_style('ticks')\n",
    "sns.set_context('talk')\n",
    "fig, ax = plt.subplots()\n",
    "# sns.set_context(\"notebook\", rc={\"lines.linewidth\": 3})\n",
    "fig.set_size_inches(10, 7)\n",
    "\n",
    "sns.scatterplot(data=result_df_description, x=\"rank\", y=\"score\", hue='dataset')\n",
    "sns.rugplot(data=result_df_description, x=\"rank\", y=\"score\", color='black')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
